# 실전 데이터 분석

## 실전 데이터 분석 예시
- 지역별 국내 휴양림 분포 (빈도분석)
- 해외 입국자 추이 확인
- 코로나 선별진료소 지도 시각화
- 서울시 지역별 미세먼지 농도


## 지역별 국내 휴양림 분포
1. 데이터 수집
    - 전국 휴양림 데이터 다운로드
    - https://www.data.go.kr
    - "전국휴양림표준데이터" 검색
    - [표준데이터셋] > "전국휴양림표준데이터"
    - [그리드] > [XLS]

2. 데이터 가공
    - 전처리
    * 데이터 항목 확인 : 휴양림명, 시도명, 휴양림구분, 휴양림면적, 수용인원수, 입장료, 숙박가능여부, 주요시설명 등
    * 비어있는 1행 삭제

    * 필요한 항목 : 휴양림명, 시도명, 휴양림구분, 휴양림면적, 수용인원수, 소재지도로명주소, 제공기관코드, 제공기관명

    * 불필요한 항목 : 입장료, 주요시설명, 관리기관명, 휴양림전화번호, 홈페이지주소, 위도, 경도, 데이터기준일

    * 새로운 항목 : "소재지도로명주소" 항목을 시와 도로 분리
        - [데이터] > [텍스트 나누기] > [구분 기호로 분리됨] > [탭, 공백] 
        - "기존 데이터를 바꾸시겠습니까?" > [확인]
        - "소재지도로명주소" → "소재지_시도명" 으로 항목명 변경
        - 시도명 아래 항목 열들은 삭제

    * 데이터 유형 변경
        - "휴양림면적", "수용인원수" : "문자형" → "숫자" 변환

    * 엑셀파일 저장 : "forest.xls"


3. 데이터 분석
    - 빈도 분석
    - 시각화

```
    # 엑셀 파일 가져오기
    install.packages("readxl")
    library(readxl)
    path <- "C:/KHM/R/code/source/"
    file <- paste0( path, "forest.xls")
    forest_data <- read_excel( file )

    # 변수명 변경
    colnames(forest_data) <- c("name", "city", "gubun", "area", "number", "stay", "city_new", "code", "codename")

    # 데이터 구조 및 앞부분 확인
    str(forest_data)
    head(forest_data)

```

``` # 시도별 휴양림 빈도분석 - freq() 함수
   library(descr)
   freq(forest_data$city, plot = T, main = 'city')
```


``` # 시도별 휴양림 빈도분석 - table() 함수
   city_table <- table(forest_data$city)
   city_table
   barplot(city_table)
```

```# 시도별 휴양림 수 구하고 내림차순 정렬하기
   library(dplyr)
   count(forest_data, city) %>% arrange(desc(n))

   # - count(forest_data, city) 
   # : 휴양림 데이터세트의 시도별 개수 → 데이터 프레임(city, n)

   # - %>% arrange( desc(n) )  
   # : 앞에서 구한 데이터 프레임의 n(개수) 기준으로 desc() 함수로 내림차순 정렬

   # 파이프 연산자 미사용
   x <- count(forest_data, city) 
   arrange( x, desc(n) )
```
    - count(forest_data, city) : 휴양림 데이터세트의 시도별 개수 → 데이터 프레임(city, n)
    - %>% arrange( desc(n) )   : 앞에서 구한 데이터 프레임의 n(개수)을 기준으로 desc() 함수로 내림차순 정렬


``` # 시도별, 소재지_시도명, 제공기관명 별로 분포 확인하기
   library(dplyr)
   count(forest_data, city) %>% arrange(desc(n))
   count(forest_data, city_new) %>% arrange(desc(n))
   count(forest_data, codename) %>% arrange(desc(n))
```


## 해외 입국자 추이 확인
1. 데이터 수집
    - 투어고 홈페이지
    - http://know.tour.go.kr
    - "2020년 입국자 수" 데이터 조회
    - [통계] > [관광객통계] > [입국관광통계]
    - 검색 조건
        - 통계유형 : 국적별 입국
        - 기간구분 : 2020년 01월 ~ 2020년 12월
    - [엑셀 내려받기] > [셀 병합 유지]


2. 데이터 가공
    1. 엑셀로 전처리
    - "국적별" 셀 [셀 병합 해제] > 2행 삭제
    - 삭제할 열 "계"
    - 삭제할 행 : (연두색 배경의 행) 아시아주, 중동, 미주, 구주, 대양주, 아프리카주, 기타, 전체
    - 엑셀 파일 저장 : entrance.xls

    2. 데이터 재구조화
    ```
    # 엑셀 파일 가져오기
    install.packages("readxl")
    library(readxl)
    path <- "C:/KHM/R/code/source/"
    file <- paste0( path, "entrance.xls")
    entrance_data <- read_excel( file )

    ```

    # 데이터 구조 및 앞부분 확인
    str(entrance_data)
    head(entrance_data)

    # 변수명 영어로 변경
    colnames(entrance_data) <- c("country", "JAN", "FEB", "MAR", "APR", "MAY", "JUN", "JUL", "AUG", "SEP", "OCT", "NOV", "DEC")

    # 국가명 띄어쓰기 제거
    entrance_data$country <- gsub(" ", "", entrance_data$country)
    head(entrance_data)
    ```

    ``` 
    # 국가 개수 확인
    entrance_data |> nrow()

    # 1월기준 상위 5개국 추출하기
    # order(-entrance_data$JAN) : 1월 기준 내림차순
    # |> head(n = 5)  :1월 기준 내림차순한 데이터의 상위 5개
    top5_country <- entrance_data[order(-entrance_data$JAN),] |> head(n = 5)
    top5_country

    ```

    ```
    # 데이터 구조 재구조화하기
    # - 열로 나열된 월별 데이터(JAN, FEB, ..)를 행으로 변환
    install.packages("reshape2")
    library(reshape2)
    top5_melt <- melt(top5_country, id.vars='country', variable.name = 'mon')
    head(top5_melt)

3. 데이터 분석
- 2020년 월별 입국자 수 
  - 데이터 변화 추이             : 선 그래프
  - 상위 5개국 국가별 변화 추이  : 막대그래프, 누적 막대 그래프
  
``` # 2020년 월별 입국자 수 - 선 그래프
   install.packages("ggplot2")
   library(ggplot2)
   ggplot(top5_melt, aes(x = mon, y = value, group = country)) + 
      geom_line(aes(color = country)) +
      ggtitle("2020년 국적별 입국자 수 변화 추이") +
      scale_y_continuous(breaks = seq(0, 500000, 50000))
```

``` # 2020년 월별 입국자 수 - 막대 그래프
   install.packages("ggplot2")
   library(ggplot2)
   ggplot(top5_melt, aes(x = mon, y = value, fill = country)) + 
      geom_bar(stat = "identity", position = "dodge") +
      ggtitle("2020년 국적별 입국자 수 변화 추이") +
      scale_y_continuous(breaks = seq(0, 500000, 50000))
```

``` # 2020년 월별 입국자 수 - 누적 그래프
   install.packages("ggplot2")
   library(ggplot2)
   ggplot(top5_melt, aes(x = mon, y = value, fill = country)) + 
      geom_bar(stat = "identity", position = "stack") +
      ggtitle("2020년 국적별 입국자 수 변화 추이") +
      scale_y_continuous(breaks = seq(0, 1000000, 50000))
```


## 코로나 선별진료소 지도 시각화하기
1. 데이터 수집
    - 공공데이터 포털
    - https://www.data.go.kr
    - "선별진료소 현황" 검색
    - [파일데이터] > "보건복지부_코로나19_선별진료소_현황"
    - [URL] > [링크] > [엑셀파일 다운로드]
    - 엑셀 파일명 : 선별진료소_현황.xls

2. 데이터 가공
    - 엑셀 파일 가져오기
    - 필요한 컬럼만 추출하기
    
    ```  
    # 엑셀 데이터 가져오기
    install.packages("readxl")
    library(readxl)
    path <- "C:/KHM/R/code/source/"
    file <- paste0(path, "선별진료소.xls")
    clinic_data <- read_excel(file)

    # 필요한 컬럼만 추출하기
    data_raw <- clinic_data[,c(2:5)]     # 2~5열의 모든 행 선택
    head(data_raw)

    # 데이터 추출 확인 및 컬럼명 변경
    names(data_raw)
    names(data_raw) <- c("state", "city", "name", "addr")
    names(data_raw)
    ```


3. 데이터 분석
    1. 빈도 분석
    2. 지도 시각화


    ```
      # 지역별(state) 선별 진료소 빈도 분석
      table(data_raw$state)

      # 지역별(state) 선별 진료소 빈도 - 막대 그래프
      barplot(table(data_raw$state))

      # 제주 선별 진료소 추출
      jeju_data <- data_raw[data_raw$state == "제주",]
      head(jeju_data)
      nrow(jeju_data)
   ```


   ```
        # 제주 선별 진료소 - 위도, 경도 가져오기
        install.packages("ggmap")
        library(ggmap)
        api_key <- "AIzaSyDa_9XcZixmykF2y95JyJCZravaIvqdPPU"
        register_google(api_key)

        # mutate_geocode()
        # - 데이터 세트의 주소를 기준으로 위도(lat),경도(lon)를 추가하는 함수
        jeju_data <- mutate_geocode(data=jeju_data, location=addr, source='google')

        View(jeju_data)


   ```

## 서울시 지역별 미세먼지 농도
- 가설 : 서울 중구와 성북구 지역은 미세먼지 농도의 평균에 차이가 있다.

- 가설 검정
 : 가설이 통계적으로 유의미한지 판단하는 과정

* t 검정 분석기법 : 두 데이터 평균의 차이를 비교하는 분석 기법

1. 데이터 수집
    - 서울특별시 대기환경 정보
    - https://cleanair.seoul.go.kr
    - [대기질 통계]
    - 측정기간 : 2023년 5월 미세먼지 전체
    - [엑셀 다운로드]

2. 데이터 가공
    1. 엑셀로 전처리
    - 1~4행 삭제
    - "구분" 셀 병합 해제
    - 1행 날짜 서식 → 텍스트 서식 변경
    - 셀 데이터 선택 → 복사 → 아래에 붙여넣기(행/열바꿈 옵션)
    - 1일 → 2023-05-01 날짜형식으로 변경
    - 1~31일 까지 자동채우기
    - 엑셀 파일 저장 : dust.xlsx

    2. 필요한 데이터 추출
    ```
        # 필요한 데이터 추출

        # 성북구, 중구 데이터만 추출

        # 결측치 확인
        
    ```



3. 데이터 분석


4. 가설 검정
    * 가설 검정 : "가설의 합당성을 판단하는 과정"
    - 통계적 추론을 사용하여 주장하고자 하는 가설이 데이터와 일치하는지를 검증하는 과정
    - 가설 검정 관련 함수
        * var.test(데이터1, 테이터2)
         : 두 개의 독립적인 집단의 분산 비교
         - p-value <= 0.05 : 의미가 있는 차이가 있다 ("분산이 다르다")
         - p-value > 0.05  : 의미가 있는 차이가 없다 ("분산이 같다") → "등분산"
        

        * t.test(데이터1, 데이터2, var.equal = 등분산성 여부)
         : 두 개의 독립적인 집단의 평균을 비교
         - p-value <= 0.05 : 의미가 있는 차이가 있다 ("평균이 다르다")
         - p-value > 0.05  : 의미가 있는 차이가 없다 ("평균이 같다")
    
    - 가설 검정 과정
    1. 귀무가설, 대립가설 설정
        - 귀무가설 : "주장이 효과가 없거나 차이가 없다"
        - 대립가설 : "주장이 효과가 있거나 차이가 있다"

    2. 유의수준 설정
        - 귀무 가설을 기각하기 위한 p-value 값의 기준 설정
        - 일반적으로 0.05
    
    3. 검정 방법 선택
        - 평균의 차이를 비교하는 경우 : t검정 분석기법 - t.test() 함수 사용
    
    4. p-value 도출
        - p-value (provavility value)
        : 계산된 통계량이 귀무가설에 부합되는 정도를 나타내는 값
        * 적절한 분석기법의 함수를 사용하여 p-value 를 결과로 얻음
    
    5. 결론 도출
        * 0.05 보다 크면 클수록 통계량이 더욱 더 귀무가설에 부합함
        * 0.05 보다 작으면 통계적으로 의미있는 차이가 있다고 판단
            → 귀무가설을 기각하고 대립가설 채택



5. 결론 도출

    - 귀무가설
        : "서울 중구와 성북구 지역은 미세먼지 농도의 평균에 차이가 없다."
    
    - t.test()
        * p-value = 0.1115 > 0.05
        → "귀무가설 채택"
        → "서울 중구와 성북구 지역은 미세먼지 농도의 평균에 차이가 없다."       